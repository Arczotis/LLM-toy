{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 将 llm_toy/src 加入 sys.path（兼容多启动位置）\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _add_src_path():\n",
        "    candidates = [\n",
        "        Path.cwd() / 'llm_toy' / 'src',           # 在项目根启动Jupyter\n",
        "        Path.cwd() / 'src',                        # 在 llm_toy 目录启动\n",
        "        Path.cwd().parent / 'llm_toy' / 'src',\n",
        "        Path.cwd().parent / 'src',\n",
        "    ]\n",
        "    # 向上回溯几层尝试\n",
        "    for base in list(Path.cwd().parents)[:3]:\n",
        "        candidates.append(base / 'llm_toy' / 'src')\n",
        "        candidates.append(base / 'src')\n",
        "    for p in candidates:\n",
        "        if (p / 'model.py').exists() and (p / 'utils.py').exists():\n",
        "            sys.path.append(str(p.resolve()))\n",
        "            print('已添加src路径:', p.resolve())\n",
        "            return str(p.resolve())\n",
        "    print('警告：未找到 llm_toy/src，请手动添加路径或调整工作目录。')\n",
        "    return None\n",
        "\n",
        "SRC_PATH = _add_src_path()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 Attention可视化：观察模型关注了哪些Token\n\n",
        "本Notebook展示如何从 GPT-2 提取Attention权重，并用heatmap进行可视化。\n\n",
        "重点概念：Self-Attention、Heads、Layers、Attention Map。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
      "# 路径已在开头插入cell中处理\n",
        "from model import SimpleGPTModel\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 准备模型与输入\n\n",
        "提示：本例不训练，仅做前向并拿到attention。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "simple = SimpleGPTModel(model_name='gpt2')\n",
        "tokenizer = simple.tokenizer\n",
        "model = simple.model.to(device)\n",
        "\n",
        "text = 'Attention is all you need, 也是Transformer的核心之一。'\n",
        "enc = tokenizer(text, return_tensors='pt')\n",
        "enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**enc, output_attentions=True)\n",
        "attentions = outputs.attentions  # List[layer] -> (batch, heads, seq, seq)\n",
        "len(attentions), [a.shape for a in attentions]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 可视化最后一层的平均Attention\n\n",
        "- 取最后一层，对所有heads取平均\n",
        "- x/y轴使用对应的tokens（注意BPE切分）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "last = attentions[-1][0]  # (heads, seq, seq) 取batch维0\n",
        "avg_attn = last.mean(dim=0).detach().cpu().numpy()  # (seq, seq)\n",
        "tokens = tokenizer.convert_ids_to_tokens(enc['input_ids'][0])\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(avg_attn, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
        "plt.title('Last Layer Avg Attention')\n",
        "plt.xlabel('Key tokens')\n",
        "plt.ylabel('Query tokens')\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 练习\n\n",
        "- 观察不同layer、不同head的Attention差异\n",
        "- 尝试不同输入句子，比较中英文token化对可视化的影响\n",
        "- 对生成过程中每一步取Attention（需要使用past_key_values与步进解码）\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
