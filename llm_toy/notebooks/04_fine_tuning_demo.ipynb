{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 将 llm_toy/src 加入 sys.path（兼容多启动位置）\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _add_src_path():\n",
        "    candidates = [\n",
        "        Path.cwd() / 'llm_toy' / 'src',           # 在项目根启动Jupyter\n",
        "        Path.cwd() / 'src',                        # 在 llm_toy 目录启动\n",
        "        Path.cwd().parent / 'llm_toy' / 'src',\n",
        "        Path.cwd().parent / 'src',\n",
        "    ]\n",
        "    # 向上回溯几层尝试\n",
        "    for base in list(Path.cwd().parents)[:3]:\n",
        "        candidates.append(base / 'llm_toy' / 'src')\n",
        "        candidates.append(base / 'src')\n",
        "    for p in candidates:\n",
        "        if (p / 'model.py').exists() and (p / 'utils.py').exists():\n",
        "            sys.path.append(str(p.resolve()))\n",
        "            print('已添加src路径:', p.resolve())\n",
        "            return str(p.resolve())\n",
        "    print('警告：未找到 llm_toy/src，请手动添加路径或调整工作目录。')\n",
        "    return None\n",
        "\n",
        "SRC_PATH = _add_src_path()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 微调入门：在指令风格数据上做小规模 Fine-tuning\n\n",
        "本Notebook展示如何对预训练的 GPT-2 做小规模 Fine-tuning，使其更贴近我们的小任务格式（如问答/指令）。\n\n",
        "注意：示例数据量极小，目的在于跑通流程与理解方法，不追求效果。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "# 路径已在开头插入cell中处理\n",
        "\n",
        "from model import SimpleGPTModel\n",
        "from trainer import LLMTrainer, SimpleDataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 构造指令风格数据\n\n",
        "我们使用极小的中文Q&A样例。实际项目请替换为更大、更干净的数据集。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pairs = [\n",
        "    ('问：什么是Transformer？', '答：一种基于Self-Attention的神经网络架构，用于序列建模。'),\n",
        "    ('问：什么是Fine-tuning？', '答：在预训练模型基础上，用下游任务数据做进一步训练。'),\n",
        "    ('问：什么是Tokenizer？', '答：把文本切分为tokens的工具，如BPE、WordPiece等方法。'),\n",
        "    ('问：如何控制生成？', '答：可以调整temperature、top-k、top-p等采样参数。'),\n",
        "]\n",
        "\n",
        "# 拼接成统一输入格式\n",
        "texts = [f'{q}\\n{a}' for q, a in pairs]\n",
        "len(texts), texts[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 初始化模型与Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "simple = SimpleGPTModel(model_name='gpt2')\n",
        "tokenizer = simple.tokenizer\n",
        "model = simple.model.to(device)\n",
        "\n",
        "# 可选：冻结大部分层，只训练输出层或最后几层（示例）\n",
        "for name, param in model.named_parameters():\n",
        "    param.requires_grad = False\n",
        "# 仅解冻最后两层block与lm_head（简化演示）\n",
        "for block in model.transformer.h[-2:]:\n",
        "    for p in block.parameters():\n",
        "        p.requires_grad = True\n",
        "for p in model.lm_head.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "sum(p.requires_grad for p in model.parameters()), sum(1 for _ in model.parameters())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 构建Dataset/DataLoader并训练\n\n",
        "- 为了演示稳定，batch小、step少、epoch=1\n",
        "- 在CPU上也能跑通（速度较慢）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ds = SimpleDataset(texts=texts, tokenizer=tokenizer, max_length=96)\n",
        "dl = DataLoader(ds, batch_size=2, shuffle=True)\n",
        "trainer = LLMTrainer(model=model, tokenizer=tokenizer, device=str(device))\n",
        "from pathlib import Path as _P2\n",
        "_cwd = _P2.cwd()\n",
        "save_dir = str((_cwd/'llm_toy'/'checkpoints'))\n",
        "trainer.train(train_dataloader=dl, val_dataloader=None, num_epochs=1, learning_rate=1e-4, save_dir=save_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 生成：观察风格贴合\n\n",
        "尝试提供一个新的问题前缀，看看模型是否以“答：”的风格续写。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompt = '问：如何理解Self-Attention？'\n",
        "ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    gen = model.generate(ids, max_length=80, do_sample=True, temperature=0.8, pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(gen[0], skip_special_tokens=True))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 思考与练习\n\n",
        "- 不冻结参数，完整微调，与部分冻结对比效果\n",
        "- 增加样本数量，观测风格拟合是否更明显\n",
        "- 尝试加入英文指令，混合多语言场景\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
