{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 将 llm_toy/src 加入 sys.path（稳健多候选）\n",
        "import sys as _sys\n",
        "from pathlib import Path as _Path\n",
        "def _add_src_path():\n",
        "    cands = [\n",
        "        _Path.cwd()/'llm_toy'/'src',\n",
        "        _Path.cwd()/'src',\n",
        "        _Path.cwd().parent/'llm_toy'/'src',\n",
        "        _Path.cwd().parent/'src',\n",
        "    ]\n",
        "    for base in list(_Path.cwd().parents)[:3]:\n",
        "        cands += [base/'llm_toy'/'src', base/'src']\n",
        "    for p in cands:\n",
        "        if (p/'model.py').exists() and (p/'utils.py').exists():\n",
        "            _sys.path.append(str(p.resolve()))\n",
        "            print('已添加src路径:', p.resolve())\n",
        "            return\n",
        "    print('警告：未找到 llm_toy/src，请手动添加路径或调整工作目录。')\n",
        "_add_src_path()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 Tokenization基础：从零训练一个BPE Tokenizer\n\n",
        "使用 tokenizers 在toy语料上训练简易BPE Tokenizer，并与GPT-2 Tokenizer对比。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from transformers import GPT2Tokenizer\n",
        "from pathlib import Path as _P2\n",
        "_cwd = _P2.cwd()\n",
        "c1 = _cwd/'llm_toy'/'data'/'tiny_corpus.txt'\n",
        "c2 = _cwd/'data'/'tiny_corpus.txt'\n",
        "c3 = _cwd.parent/'llm_toy'/'data'/'tiny_corpus.txt'\n",
        "data_path = c1 if c1.exists() else (c2 if c2.exists() else c3)\n",
        "save_path = (_cwd/'llm_toy'/'data'/'bpe_tokenizer.json')\n",
        "(data_path.exists(), str(data_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "trainer = BpeTrainer(vocab_size=2000, special_tokens=['[PAD]','[UNK]','[BOS]','[EOS]'])\n",
        "tokenizer.train(files=[str(data_path)], trainer=trainer)\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single='[BOS] $A [EOS]',\n",
        "    pair='[BOS] $A [EOS] $B:1 [EOS]:1',\n",
        "    special_tokens=[('[BOS]', tokenizer.token_to_id('[BOS]')), ('[EOS]', tokenizer.token_to_id('[EOS]'))]\n",
        ")\n",
        "tokenizer.save(str(save_path))\n",
        "(str(save_path), save_path.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "sample = '自然语言处理让计算机理解文本，Tokenization是第一步。'\n",
        "enc = tokenizer.encode(sample)\n",
        "(enc.tokens, enc.ids, tokenizer.decode(enc.ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "gpt2_tok = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "g_ids = gpt2_tok.encode(sample)\n",
        "g_tokens = gpt2_tok.convert_ids_to_tokens(g_ids)\n",
        "g_tokens[:20]\n"
      ]
    }
  ],
  "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3"}},
  "nbformat": 4,
  "nbformat_minor": 5
}
