{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 将 llm_toy/src 加入 sys.path（兼容多启动位置）\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _add_src_path():\n",
        "    candidates = [\n",
        "        Path.cwd() / 'llm_toy' / 'src',           # 在项目根启动Jupyter\n",
        "        Path.cwd() / 'src',                        # 在 llm_toy 目录启动\n",
        "        Path.cwd().parent / 'llm_toy' / 'src',\n",
        "        Path.cwd().parent / 'src',\n",
        "    ]\n",
        "    # 向上回溯几层尝试\n",
        "    for base in list(Path.cwd().parents)[:3]:\n",
        "        candidates.append(base / 'llm_toy' / 'src')\n",
        "        candidates.append(base / 'src')\n",
        "    for p in candidates:\n",
        "        if (p / 'model.py').exists() and (p / 'utils.py').exists():\n",
        "            sys.path.append(str(p.resolve()))\n",
        "            print('已添加src路径:', p.resolve())\n",
        "            return str(p.resolve())\n",
        "    print('警告：未找到 llm_toy/src，请手动添加路径或调整工作目录。')\n",
        "    return None\n",
        "\n",
        "SRC_PATH = _add_src_path()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 训练入门：用小数据微训练 GPT-2 (教学版)\n\n",
        "本Notebook聚焦动手实践：在一个极小toy数据集上跑通完整训练流程，理解关键步骤与参数。\n\n",
        "说明与注释为中文，专有名词如 GPT-2、Tokenizer、DataLoader、Fine-tuning 等保持英文原名。\n\n",
        "提示：首次使用 `transformers` 加载模型可能需要联网下载权重；若无网络，请确保本机已有缓存。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 环境与导入\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 便于直接导入 llm_toy/src（已在开头插入path逻辑）\n",
        "import sys\n",
        "\n",
        "from model import SimpleGPTModel\n",
        "from trainer import LLMTrainer, SimpleDataset\n",
        "from trainer import create_sample_data\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 准备数据\n\n",
        "我们混合两类toy数据：\n",
        "- `create_sample_data()` 生成的英文短句\n",
        "- `llm_toy/data/tiny_corpus.txt` 的中文段落\n\n",
        "目标：快速拼出一个小语料，演示 Tokenizer、Dataset、DataLoader 的基本用法。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 读取 toy 语料（解析路径多候选）\n",
        "from pathlib import Path as _P2\n",
        "_cwd = _P2.cwd()\n",
        "_cands = [\n",
        "    _cwd/'llm_toy'/'data'/'tiny_corpus.txt',\n",
        "    _cwd/'data'/'tiny_corpus.txt',\n",
        "    _cwd.parent/'llm_toy'/'data'/'tiny_corpus.txt',\n",
        "]\n",
        "for base in list(_cwd.parents)[:3]:\n",
        "    _cands.append(base/'llm_toy'/'data'/'tiny_corpus.txt')\n",
        "tiny_path = None\n",
        "for _p in _cands:\n",
        "    if _p.exists(): tiny_path = _p; break\n",
        "if tiny_path is None:\n",
        "    print('警告：未找到 tiny_corpus.txt，仅使用英文sample数据。')\n",
        "corpus_text = tiny_path.read_text(encoding='utf-8').strip().split('\n') if tiny_path.exists() else []\n",
        "\n",
        "sample_texts = create_sample_data()\n",
        "texts = corpus_text + sample_texts\n",
        "len(texts), texts[:2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 初始化模型与Tokenizer\n\n",
        "说明：`SimpleGPTModel` 封装了 GPT-2 与 Tokenizer。首次加载可能需要下载权重。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    simple = SimpleGPTModel(model_name='gpt2')\n",
        "except Exception as e:\n",
        "    print('加载模型失败：', e)\n",
        "    print('若无网络，请确认本地已缓存 gpt2 权重或稍后联网重试。')\n",
        "    raise\n",
        "\n",
        "tokenizer = simple.tokenizer\n",
        "base_model = simple.model\n",
        "base_model.to(device); device\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 构建Dataset与DataLoader\n\n",
        "- `SimpleDataset` 会将文本用 Tokenizer 编码为 `input_ids` 与 `attention_mask`\n",
        "- `max_length` 控制截断/填充长度；演示用 64 即可\n",
        "- `batch_size` 用 4，CPU 也可运行\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_ds = SimpleDataset(texts=texts, tokenizer=tokenizer, max_length=64)\n",
        "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
        "len(train_ds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 训练循环（教学化）\n\n",
        "我们使用 `LLMTrainer` 封装：\n",
        "- Optimizer: AdamW\n",
        "- Scheduler: Linear Warmup\n",
        "- 训练 epoch：1（示例足够）\n\n",
        "练习：尝试修改 `learning_rate` 与 `num_epochs`，观察loss变化。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer = LLMTrainer(model=base_model, tokenizer=tokenizer, device=str(device))\n",
        "history = trainer.train(\n",
        "    train_dataloader=train_dl,\n",
        "    val_dataloader=None,\n",
        "    num_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=0,\n",
        "    # 保存路径（默认放到项目下 llm_toy/checkpoints）\n",
        "    save_dir=str((_cwd/'llm_toy'/'checkpoints'))\n",
        ")\n",
        "history[:2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 简单生成（Sanity Check）\n\n",
        "使用训练后的权重进行短文本生成。注意：少量步骤训练不会带来显著能力提升，重在跑通流程。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompt = '自然语言处理的核心是'\n",
        "inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    out_ids = base_model.generate(\n",
        "        inputs, max_length=50, do_sample=True, temperature=0.8,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "print(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 练习建议\n\n",
        "- 改变 `batch_size`、`max_length`、`temperature` 并记录现象\n",
        "- 替换语料，尝试更偏向某个领域的文本\n",
        "- 增加 `num_epochs` 看loss下降趋势（CPU会慢，建议少量步数）\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
