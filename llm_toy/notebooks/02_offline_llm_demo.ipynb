{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T11:27:42.224251Z",
     "start_time": "2025-11-13T11:27:42.219380Z"
    }
   },
   "source": [
    "# Â∞Ü llm_toy/src Âä†ÂÖ• sys.pathÔºàÂÖºÂÆπÂ§öÂêØÂä®‰ΩçÁΩÆÔºâ\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def _add_src_path():\n",
    "    candidates = [\n",
    "        Path.cwd() / 'llm_toy' / 'src',           # Âú®È°πÁõÆÊ†πÂêØÂä®Jupyter\n",
    "        Path.cwd() / 'src',                        # Âú® llm_toy ÁõÆÂΩïÂêØÂä®\n",
    "        Path.cwd().parent / 'llm_toy' / 'src',\n",
    "        Path.cwd().parent / 'src',\n",
    "    ]\n",
    "    # Âêë‰∏äÂõûÊ∫ØÂá†Â±ÇÂ∞ùËØï\n",
    "    for base in list(Path.cwd().parents)[:3]:\n",
    "        candidates.append(base / 'llm_toy' / 'src')\n",
    "        candidates.append(base / 'src')\n",
    "    for p in candidates:\n",
    "        if (p / 'model.py').exists() and (p / 'utils.py').exists():\n",
    "            sys.path.append(str(p.resolve()))\n",
    "            print('Â∑≤Ê∑ªÂä†srcË∑ØÂæÑ:', p.resolve())\n",
    "            return str(p.resolve())\n",
    "    print('Ë≠¶ÂëäÔºöÊú™ÊâæÂà∞ llm_toy/srcÔºåËØ∑ÊâãÂä®Ê∑ªÂä†Ë∑ØÂæÑÊàñË∞ÉÊï¥Â∑•‰ΩúÁõÆÂΩï„ÄÇ')\n",
    "    return None\n",
    "\n",
    "SRC_PATH = _add_src_path()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â∑≤Ê∑ªÂä†srcË∑ØÂæÑ: /home/arczotis/PycharmProjects/JupyterProject/llm_toy/src\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline LLM Demo\n",
    "\n",
    "This notebook demonstrates the offline-friendly LLM functionality using a fallback model when GPT-2 cannot be downloaded.\n",
    "\n",
    "The offline model provides the same interface as the real GPT-2 model but returns simulated responses for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T11:28:21.512194Z",
     "start_time": "2025-11-13T11:28:19.852073Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../src'))\n",
    "\n",
    "from offline_model import create_model\n",
    "from utils import set_seed, get_device, print_gpu_memory\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Get device\n",
    "device = get_device()\n",
    "print_gpu_memory()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n",
      "Using CUDA device: NVIDIA GeForce RTX 2070\n",
      "CUDA version: 12.8\n",
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory cached: 0.00 GB\n",
      "GPU memory total: 7.79 GB\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model with Automatic Fallback\n",
    "\n",
    "The `create_model()` function will:\n",
    "1. Try to load the real GPT-2 model from Hugging Face\n",
    "2. Automatically fall back to offline demo mode if network fails\n",
    "3. Provide clear messaging about which mode is being used"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T11:30:28.550013Z",
     "start_time": "2025-11-13T11:30:28.543688Z"
    }
   },
   "source": [
    "# Initialize the model with automatic fallback\n",
    "print(\"Loading GPT model with automatic fallback...\")\n",
    "gpt_model = create_model(model_name=\"gpt2\")\n",
    "\n",
    "# Get model info\n",
    "model_info = gpt_model.get_model_info()\n",
    "print(\"\\nModel Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"{key}: {value}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT model with automatic fallback...\n",
      "‚ö†Ô∏è  Could not import SimpleGPTModel - using offline demo mode\n",
      "Using device: cuda\n",
      "‚úÖ Initialized OfflineGPTModel (Demo Mode)\n",
      "   Vocabulary size: 315 words\n",
      "   Simulating: gpt2 architecture\n",
      "\n",
      "Model Information:\n",
      "model_name: gpt2-offline-demo\n",
      "vocab_size: 50257\n",
      "hidden_size: 768\n",
      "num_layers: 12\n",
      "num_heads: 12\n",
      "device: cuda\n",
      "mode: OFFLINE_DEMO\n",
      "warning: This is a demonstration model - responses are simulated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arczotis/PycharmProjects/JupyterProject/llm_toy/src/offline_model.py:219: UserWarning: ‚ö†Ô∏è  Running in OFFLINE/DEMO MODE - This is a placeholder model for educational purposes!\n",
      "   The responses are simulated and not from a real GPT model.\n",
      "  return OfflineGPTModel(model_name)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force Offline Mode (Optional)\n",
    "\n",
    "You can also force offline mode if you want to use the demo model intentionally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force offline mode\n",
    "print(\"Forcing offline demo mode...\")\n",
    "offline_model = create_model(model_name=\"gpt2\", force_offline=True)\n",
    "\n",
    "# Get model info\n",
    "offline_info = offline_model.get_model_info()\n",
    "print(\"\\nOffline Model Information:\")\n",
    "for key, value in offline_info.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Examples\n",
    "\n",
    "The offline model provides contextually relevant responses based on the input prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text generation with various prompts\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"Machine learning can help us\",\n",
    "    \"In the world of technology,\",\n",
    "    \"The most important programming language for AI is\"\n",
    "]\n",
    "\n",
    "print(\"Text Generation Examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    # Generate text with different parameters\n",
    "    generated = gpt_model.generate_text(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Different Generation Parameters\n",
    "\n",
    "The offline model respects the same parameters as the real model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different generation parameters\n",
    "test_prompt = \"Deep learning is\"\n",
    "\n",
    "print(\"Testing Different Generation Parameters:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Low temperature (more deterministic)\n",
    "print(\"\\nLow Temperature (0.3) - More deterministic:\")\n",
    "result1 = gpt_model.generate_text(test_prompt, max_length=30, temperature=0.3)\n",
    "print(result1)\n",
    "\n",
    "# High temperature (more creative)\n",
    "print(\"\\nHigh Temperature (1.2) - More creative:\")\n",
    "result2 = gpt_model.generate_text(test_prompt, max_length=30, temperature=1.2)\n",
    "print(result2)\n",
    "\n",
    "# Different max lengths\n",
    "print(\"\\nShort generation (max_length=15):\")\n",
    "result3 = gpt_model.generate_text(test_prompt, max_length=15, temperature=0.7)\n",
    "print(result3)\n",
    "\n",
    "print(\"\\nLong generation (max_length=80):\")\n",
    "result4 = gpt_model.generate_text(test_prompt, max_length=80, temperature=0.7)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Offline Model\n",
    "\n",
    "The offline model provides:\n",
    "\n",
    "1. **Same Interface**: All methods from SimpleGPTModel are available\n",
    "2. **Contextual Responses**: Generates responses based on prompt content\n",
    "3. **Parameter Respect**: Honors temperature, max_length, and do_sample parameters\n",
    "4. **Educational Value**: Provides realistic-looking responses for learning purposes\n",
    "5. **Clear Warnings**: Always informs users when running in demo mode\n",
    "\n",
    "**Note**: The responses are simulated and not from a real language model. They are designed to be educational and demonstrate how the real model would behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we've learned\n",
    "print(\"üìö Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Offline model successfully loaded and tested\")\n",
    "print(\"‚úÖ Same interface as real GPT-2 model\")\n",
    "print(\"‚úÖ Contextually aware responses\")\n",
    "print(\"‚úÖ Parameter-sensitive generation\")\n",
    "print(\"‚úÖ Educational value maintained\")\n",
    "print(\"‚úÖ Clear offline mode warnings\")\n",
    "\n",
    "print(\"\\nüéØ The offline model is ready for educational use!\")\n",
    "print(\"   Notebooks can now run without internet connectivity.\")\n",
    "print(\"   Students can learn LLM concepts even in offline environments.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
