{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Troubleshooting Guide\n",
    "\n",
    "This notebook helps diagnose and fix common setup issues with CUDA and PyTorch.\n",
    "\n",
    "## üîç Current Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NVIDIA driver\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ NVIDIA driver is working\")\n",
    "        print(\"First few lines of nvidia-smi output:\")\n",
    "        print('\\n'.join(result.stdout.split('\\n')[:5]))\n",
    "    else:\n",
    "        print(\"‚ùå nvidia-smi failed:\")\n",
    "        print(result.stderr)\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå nvidia-smi not found - NVIDIA driver may not be installed\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running nvidia-smi: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Common Issues and Solutions\n",
    "\n",
    "### Issue 1: CUDA Driver Mismatch\n",
    "\n",
    "**Symptoms:**\n",
    "- `CUDA available: False` even with NVIDIA GPU\n",
    "- `Driver/library version mismatch` errors\n",
    "- `NVML initialization failed`\n",
    "\n",
    "**Solutions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what CUDA version PyTorch expects vs what's installed\n",
    "print(\"PyTorch CUDA version:\", torch.version.cuda)\n",
    "\n",
    "# Check if CUDA runtime libraries are available\n",
    "cuda_paths = [\n",
    "    \"/usr/local/cuda/lib64/libcudart.so\",\n",
    "    \"/usr/lib/x86_64-linux-gnu/libcudart.so\",\n",
    "    \"/usr/local/cuda/lib64/libcudart.so.*\"\n",
    "]\n",
    "\n",
    "import glob\n",
    "for path in cuda_paths:\n",
    "    matches = glob.glob(path)\n",
    "    if matches:\n",
    "        print(f\"Found CUDA libraries: {matches}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"No CUDA runtime libraries found in standard locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 2: CPU-Only Mode Fallback\n",
    "\n",
    "If CUDA is not available, you can still learn LLM concepts using CPU (though slower):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CPU mode\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a simple tensor operation\n",
    "x = torch.randn(100, 100, device=device)\n",
    "y = torch.matmul(x, x.T)\n",
    "print(f\"CPU computation successful: {y.shape}\")\n",
    "\n",
    "# Check memory usage\n",
    "import psutil\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"System memory: {memory.used / 1024**3:.1f}GB used / {memory.total / 1024**3:.1f}GB total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 3: Reinstalling PyTorch with Correct CUDA Version"
",
    "\n",
    "If you need to reinstall PyTorch with a different CUDA version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your NVIDIA driver version\n",
    "print(\"To find your NVIDIA driver version, run in terminal:\")\n",
    "print(\"cat /proc/driver/nvidia/version\")\n",
    "print(\"\\nOr check the driver version in the output above\")\n",
    "\n",
    "print(\"\\nFor RTX 2070, recommended CUDA versions:\")\n",
    "print(\"- CUDA 11.8 (very stable)\")\n",
    "print(\"- CUDA 12.1 (newer, also good)\")\n",
    "print(\"- Avoid CUDA 12.2+ (may have compatibility issues)\")\n",
    "\n",
    "print(\"\\nTo reinstall PyTorch with CUDA 11.8:\")\n",
    "print(\"pip uninstall torch torchvision torchaudio\")\n",
    "print(\"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Manual CUDA Installation Steps\n",
    "\n",
    "If automatic installation doesn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step CUDA installation guide\n",
    "cuda_install_steps = [\n",
    "    \"1. Check your GPU driver version\",\n",
    "    \"   nvidia-smi # or cat /proc/driver/nvidia/version\",\n",
    "    \"\",\n",
    "    \"2. Download appropriate CUDA toolkit\",\n",
    "    \"   Visit: https://developer.nvidia.com/cuda-toolkit-archive\",\n",
    "    \"   Choose version compatible with your driver\",\n",
    "    \"\",\n",
    "    \"3. Install CUDA toolkit\",\n",
    "    \"   sudo apt update\",\n",
    "    \"   sudo apt install cuda-toolkit-11-8  # for CUDA 11.8\",\n",
    "    \"\",\n",
    "    \"4. Set environment variables\",\n",
    "    \"   export PATH=/usr/local/cuda-11.8/bin:$PATH\",\n",
    "    \"   export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH\",\n",
    "    \"\",\n",
    "    \"5. Install PyTorch with matching CUDA\",\n",
    "    \"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\",\n",
    "    \"\",\n",
    "    \"6. Verify installation\",\n",
    "    \"   python -c \"import torch; print(torch.cuda.is_available())\"\"\",\n",
    "]\n",
    "\n",
    "print(\"CUDA Installation Steps:\")\n",
    "for step in cuda_install_steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Continue Learning (CPU Mode)\n",
    "\n",
    "Even without GPU, you can still learn LLM concepts:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that core functionality works on CPU\n",
    "print(\"Testing core LLM functionality on CPU...\")\n",
    "\n",
    "# This will work even without GPU\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "try:\n",
    "    from utils import set_seed, create_sample_data\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_texts = create_sample_data()\n",
    "    print(f\"‚úÖ Created {len(sample_texts)} sample texts\")\n",
    "    \n",
    "    # Test basic text processing\n",
    "    from transformers import GPT2Tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    text = \"Hello, world! This is a test.\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    \n",
    "    print(f\"‚úÖ Tokenization works: '{text}' -> {tokens} -> '{decoded}'\")\n",
    "    \n",
    "    print(\"\\nüéâ Core functionality works! You can proceed with learning.\")\n",
    "    print(\"Note: Training will be slower on CPU, but concepts are the same.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Check your internet connection for model downloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Next Steps\n",
    "\n",
    "Choose your path:\n",
    "\n",
    "1. **Fix CUDA Issues** ‚Üí Follow the installation steps above\n",
    "2. **Continue on CPU** ‚Üí Proceed to `01_pytorch_setup.ipynb` (it will work on CPU)\n",
    "3. **Get Help** ‚Üí Check the project README or seek assistance\n",
    "\n",
    "Remember: The learning concepts are identical whether you use CPU or GPU!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}