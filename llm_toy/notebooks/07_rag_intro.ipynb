{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 将 llm_toy/src 加入 sys.path（稳健多候选）\n",
        "import sys as _sys\n",
        "from pathlib import Path as _Path\n",
        "def _add_src_path():\n",
        "    cands = [\n",
        "        _Path.cwd()/'llm_toy'/'src',\n",
        "        _Path.cwd()/'src',\n",
        "        _Path.cwd().parent/'llm_toy'/'src',\n",
        "        _Path.cwd().parent/'src',\n",
        "    ]\n",
        "    for base in list(_Path.cwd().parents)[:3]:\n",
        "        cands += [base/'llm_toy'/'src', base/'src']\n",
        "    for p in cands:\n",
        "        if (p/'model.py').exists() and (p/'utils.py').exists():\n",
        "            _sys.path.append(str(p.resolve()))\n",
        "            print('已添加src路径:', p.resolve())\n",
        "            return\n",
        "    print('警告：未找到 llm_toy/src，请手动添加路径或调整工作目录。')\n",
        "_add_src_path()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 07 RAG入门：Retrieval-Augmented Generation（检索增强生成）\n\n",
        "本Notebook演示最小RAG流程：使用 TF-IDF 检索文档，并把结果拼接到Prompt中交给 GPT-2 生成回答。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from model import SimpleGPTModel\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 构建检索语料（Corpus）\n\n",
        "混合 tiny_corpus.txt 与少量额外句子，形成一个小型知识库（Doc集合）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 解析 tiny_corpus.txt 路径（多候选）\n",
        "from pathlib import Path as _P2\n",
        "_cwd = _P2.cwd()\n",
        "_cands = [\n",
        "    _cwd/'llm_toy'/'data'/'tiny_corpus.txt',\n",
        "    _cwd/'data'/'tiny_corpus.txt',\n",
        "    _cwd.parent/'llm_toy'/'data'/'tiny_corpus.txt',\n",
        "]\n",
        "for base in list(_cwd.parents)[:3]:\n",
        "    _cands.append(base/'llm_toy'/'data'/'tiny_corpus.txt')\n",
        "tiny_path = None\n",
        "for _p in _cands:\n",
        "    if _p.exists(): tiny_path = _p; break\n",
        "\n",
        "lines = []\n",
        "if tiny_path and tiny_path.exists():\n",
        "    lines += tiny_path.read_text(encoding='utf-8').strip().split('\n')\n",
        "extra = [\n",
        "    'Transformer 使用 Self-Attention 捕捉序列中远距离依赖。',\n",
        "    'Fine-tuning 可以让预训练模型适配特定任务或风格。',\n",
        "    'Tokenization 将文本切分为 tokens，是NLP管线的基础环节。',\n",
        "    'Top-k 与 Top-p 采样影响生成的多样性与保守性。',\n",
        "]\n",
        "corpus = [s for s in (lines + extra) if s and isinstance(s, str)]\n",
        "len(corpus), corpus[:3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 构建TF-IDF检索器\n\n",
        "拟合语料得到 doc-term 矩阵，查询时计算 cosine 相似度并返回Top-k文档。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def retrieve(query: str, k: int = 3):\n",
        "    q = vectorizer.transform([query])\n",
        "    sims = cosine_similarity(q, X)[0]\n",
        "    idx = np.argsort(-sims)[:k]\n",
        "    return [(int(i), float(sims[i]), corpus[int(i)]) for i in idx]\n",
        "\n",
        "retrieve('什么是 Transformer 和 Self-Attention？', k=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 组装Prompt并用生成模型作答\n\n",
        "把检索到的文档作为“已知信息”拼到Prompt中，提示模型基于此作答。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "simple = SimpleGPTModel(model_name='gpt2')\n",
        "tok = simple.tokenizer\n",
        "gen_model = simple.model.to(device)\n",
        "\n",
        "def build_prompt(query: str, ctx_list):\n",
        "    ctx_lines = [f'{i+1}. {c}' for i, (_, _, c) in enumerate(ctx_list)]\n",
        "    ctx_block = '\\n'.join(ctx_lines)\n",
        "    tmpl = (\n",
        "        '请基于以下已知信息简洁回答用户问题。'\n" ,
        "        '\\n已知信息：\\n' + ctx_block +\n",
        "        '\\n问题：' + query + '\\n答案：'\n",
        "    )\n",
        "    return tmpl\n",
        "\n",
        "def generate_answer(prompt: str, max_new_tokens=80, temperature=0.7):\n",
        "    input_ids = tok.encode(prompt, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        out = gen_model.generate(\n",
        "            input_ids,\n",
        "            max_length=min(1024, input_ids.shape[1] + max_new_tokens),\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tok.eos_token_id\n",
        "        )\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "query = '如何简单解释 Transformer 的核心思想？'\n",
        "ctx = retrieve(query, k=3)\n",
        "prompt = build_prompt(query, ctx)\n",
        "print('--- Prompt ---')\n",
        "print(prompt)\n",
        "print('\n--- Answer ---')\n",
        "print(generate_answer(prompt))\n"
      ]
    }
  ],
  "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3"}},
  "nbformat": 4,
  "nbformat_minor": 5
}
