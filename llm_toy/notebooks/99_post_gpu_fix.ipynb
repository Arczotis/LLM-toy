{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPUä¿®å¤åéªŒè¯\n",
    "\n",
    "è¿è¡Œè¿™ä¸ªç¬”è®°æœ¬æ¥éªŒè¯GPUä¿®å¤æ˜¯å¦æˆåŠŸ\n",
    "\n",
    "## ğŸ” é©±åŠ¨çŠ¶æ€æ£€æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=== NVIDIAé©±åŠ¨çŠ¶æ€ ===\")\n",
    "\n",
    "# æ£€æŸ¥nvidia-smi\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… nvidia-smi å·¥ä½œæ­£å¸¸\")\n",
    "        print(\"è¾“å‡ºå‰10è¡Œ:\")\n",
    "        print('\\n'.join(result.stdout.split('\\n')[:10]))\n",
    "    else:\n",
    "        print(\"âŒ nvidia-smi å¤±è´¥:\")\n",
    "        print(result.stderr)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ nvidia-smi é”™è¯¯: {e}\")\n",
    "\n",
    "# æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬\n",
    "print(\"\\n=== é©±åŠ¨ç‰ˆæœ¬ä¿¡æ¯ ===\")\n",
    "try:\n",
    "    with open('/proc/driver/nvidia/version', 'r') as f:\n",
    "        version_info = f.read()\n",
    "        print(version_info)\n",
    "except:\n",
    "    print(\"æ— æ³•è¯»å–é©±åŠ¨ç‰ˆæœ¬ä¿¡æ¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ PyTorch CUDAæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"=== PyTorch CUDAæµ‹è¯• ===\")\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "print(f\"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"è®¾å¤‡ {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  æ€»å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ CUDAä¸å¯ç”¨ - æ£€æŸ¥é©±åŠ¨å®‰è£…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUè®¡ç®—æµ‹è¯•\n",
    "if torch.cuda.is_available():\n",
    "    print(\"=== GPUè®¡ç®—æµ‹è¯• ===\")\n",
    "    \n",
    "    # åˆ›å»ºå¤§çŸ©é˜µ\n",
    "    size = 2000\n",
    "    x = torch.randn(size, size, device='cuda')\n",
    "    y = torch.randn(size, size, device='cuda')\n",
    "    \n",
    "    print(f\"åˆ›å»ºçŸ©é˜µ: {size}x{size}\")\n",
    "    print(f\"å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # çŸ©é˜µä¹˜æ³•\n",
    "    import time\n",
    "    start = time.time()\n",
    "    z = torch.matmul(x, y)\n",
    "    torch.cuda.synchronize()  # ç­‰å¾…GPUå®Œæˆ\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"çŸ©é˜µä¹˜æ³•å®Œæˆ\")\n",
    "    print(f\"è®¡ç®—æ—¶é—´: {end - start:.3f} ç§’\")\n",
    "    print(f\"ç»“æœå½¢çŠ¶: {z.shape}\")\n",
    "    print(f\"æœ€ç»ˆå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del x, y, z\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "else:\n",
    "    print(\"è·³è¿‡GPUæµ‹è¯• - CUDAä¸å¯ç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¥ ç¥ç»ç½‘ç»œè®­ç»ƒæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"=== ç®€å•ç¥ç»ç½‘ç»œè®­ç»ƒæµ‹è¯• ===\")\n",
    "    \n",
    "    # åˆ›å»ºç®€å•çš„ç½‘ç»œ\n",
    "    class SimpleNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "    \n",
    "    # ç½‘ç»œå‚æ•°\n",
    "    input_size = 784\n",
    "    hidden_size = 128\n",
    "    output_size = 10\n",
    "    \n",
    "    # åˆ›å»ºç½‘ç»œå’Œä¼˜åŒ–å™¨\n",
    "    net = SimpleNet(input_size, hidden_size, output_size).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    \n",
    "    print(f\"ç½‘ç»œç»“æ„: {input_size} -> {hidden_size} -> {output_size}\")\n",
    "    print(f\"æ€»å‚æ•°: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "    \n",
    "    # ç”Ÿæˆéšæœºæ•°æ®\n",
    "    batch_size = 64\n",
    "    X = torch.randn(batch_size, input_size).cuda()\n",
    "    y = torch.randint(0, output_size, (batch_size,)).cuda()\n",
    "    \n",
    "    # è®­ç»ƒå‡ ä¸ªepoch\n",
    "    net.train()\n",
    "    for epoch in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 0:\n",
    "            print(f\"ç¬¬1ä¸ªepochæŸå¤±: {loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"æœ€ç»ˆæŸå¤±: {loss.item():.4f}\")\n",
    "    print(\"âœ… GPUè®­ç»ƒæµ‹è¯•å®Œæˆ!\")\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del net, X, y\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "else:\n",
    "    print(\"è·³è¿‡ç¥ç»ç½‘ç»œæµ‹è¯• - CUDAä¸å¯ç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ é¡¹ç›®é›†æˆæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•é¡¹ç›®ä¸­çš„åŠŸèƒ½\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../src'))\n",
    "\n",
    "print(\"=== é¡¹ç›®é›†æˆæµ‹è¯• ===\")\n",
    "\n",
    "try:\n",
    "    from utils import set_seed, get_device, print_gpu_memory\n",
    "    from trainer import create_sample_data\n",
    "    \n",
    "    # è®¾ç½®éšæœºç§å­\n",
    "    set_seed(42)\n",
    "    \n",
    "    # è·å–è®¾å¤‡\n",
    "    device = get_device()\n",
    "    print(f\"é¡¹ç›®æ£€æµ‹è®¾å¤‡: {device}\")\n",
    "    \n",
    "    # æ‰“å°GPUå†…å­˜\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # åˆ›å»ºæ ·æœ¬æ•°æ®\n",
    "    sample_texts = create_sample_data()\n",
    "    print(f\"âœ… åˆ›å»ºæ ·æœ¬æ•°æ®: {len(sample_texts)} æ¡æ–‡æœ¬\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"ğŸ‰ GPUä¿®å¤æˆåŠŸï¼å¯ä»¥åŠ é€Ÿè®­ç»ƒäº†ï¼\")\n",
    "    else:\n",
    "        print(\"âš ï¸ GPUä»ç„¶ä¸å¯ç”¨ï¼Œä½†å¯ä»¥ç»§ç»­CPUå­¦ä¹ \")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ é¡¹ç›®æµ‹è¯•å¤±è´¥: {e}\")\n",
    "    print(\"æ£€æŸ¥é¡¹ç›®æ–‡ä»¶å®Œæ•´æ€§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ä¸‹ä¸€æ­¥è¡ŒåŠ¨æ¸…å•\n",
    "\n",
    "æ ¹æ®æµ‹è¯•ç»“æœé€‰æ‹©ä¸‹ä¸€æ­¥ï¼š\n",
    "\n",
    "### å¦‚æœGPUä¿®å¤æˆåŠŸ âœ…\n",
    "1. è¿è¡Œ `python main.py --test generation` æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ\n",
    "2. å¼€å§‹ `notebooks/02_simple_llm_demo.ipynb` å­¦ä¹ \n",
    "3. å°è¯•è®­ç»ƒæ›´å¤æ‚çš„æ¨¡å‹\n",
    "\n",
    "### å¦‚æœGPUä»ç„¶æœ‰é—®é¢˜ âŒ\n",
    "1. æ£€æŸ¥é©±åŠ¨å®‰è£…æ—¥å¿—\n",
    "2. å°è¯•ä¸åŒçš„é©±åŠ¨ç‰ˆæœ¬ï¼ˆ535/570/470ï¼‰\n",
    "3. ç»§ç»­CPUæ¨¡å¼å­¦ä¹ ï¼ˆå®Œå…¨å¯è¡Œï¼‰\n",
    "4. å¯»æ±‚ç¤¾åŒºå¸®åŠ©\n",
    "\n",
    "## ğŸ”— æœ‰ç”¨å‘½ä»¤\n",
    "\n",
    "```bash\n",
    "# æ£€æŸ¥é©±åŠ¨çŠ¶æ€\n",
    "nvidia-smi\n",
    "cat /proc/driver/nvidia/version\n",
    "lsmod | grep nvidia\n",
    "\n",
    "# é‡æ–°å®‰è£…é©±åŠ¨\n",
    "sudo ./fix_gpu_quick.sh\n",
    "\n",
    "# æµ‹è¯•PyTorch\n",
    "python -c \"import torch; print(torch.cuda.is_available())\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}